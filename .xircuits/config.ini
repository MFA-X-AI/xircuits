[DEV]
BASE_PATH = xai_components

[SERVER]
IP_ADD = http://127.0.0.1
PORT = 5000

[REMOTE_EXECUTION]
# Main run types to do remote execution using subprocess module(eg. Spark submit etc.)
# This run types will be shown on the toolbar dropdown
# Separate each run type in a newline
# Uncomment 'TEST' to add a new run type
RUN_TYPES = SPARK
        ;     TEST 

[RUN_TYPES]
# This different types will be shown on each run types
# Separate each run types and insert every run configurations in a newline
SPARK = CPU 
        GPU
        VE

# Uncomment below to add some config in TEST's run type
; TEST = EG
;        EG2

[CONFIGURATION]
# Separate each config for each run types
# Make sure each config name is the same as the above. The name inside the bracket.
# Note: Create a unique name for each config types
# Note: Make sure every criteria is FILLED
[EG]
name =  EG
command = Testing
msg = Testing
url = http://localhost:8088/

[EG2]
name =  EG2
command = Testing2
msg = Testing2
url = http://localhost:8088/

[CPU]
name =  CPU
command = $SPARK_HOME/bin/spark-submit \
        --py-files venv_pyspark_cyclone.zip \
        --archives venv_pyspark_cyclone.zip \
        --master yarn \
        --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH='/usr/local/cuda-11.2/targets/x86_64-linux/lib/:$LD_LIBRARY_PATH' \ 
        --num-executors=8 --executor-cores=1 --executor-memory=10G --driver-memory=10G \
        --name CPU_CPU_mode \
        --deploy-mode cluster \
        --conf spark.rpc.message.maxSize=1024 \
        --conf spark.driver.maxResultSize=10G 
msg = Running Spark Submit using CPU 
url = http://localhost:8088/

[GPU]
name = GPU
command = $SPARK_HOME/bin/spark-submit \
        --py-files venv_pyspark_cyclone.zip \
        --archives venv_pyspark_cyclone.zip \
        --master yarn \
        --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH='/usr/local/cuda-11.2/targets/x86_64-linux/lib/:$LD_LIBRARY_PATH' \
        --num-executors=16 --executor-cores=1 --executor-memory=7G --driver-memory=8G \
        --name VE_GPU_mode \
        --deploy-mode cluster \
        --jars /opt/cyclone/spark-cyclone-sql-plugin.jar \
        --conf spark.executor.extraClassPath=/opt/cyclone/spark-cyclone-sql-plugin.jar \
        --conf spark.plugins=com.nec.spark.AuroraSqlPlugin \
        --conf spark.com.nec.spark.kernel.directory=/opt/spark/work/cyclone \
        --conf spark.sql.columnVector.offheap.enabled=true \
        --conf spark.executor.resource.ve.amount=2 \
        --conf spark.executor.resource.ve.discoveryScript=/opt/spark/getVEsResources.sh \
        --conf spark.executorEnv.VE_OMP_NUM_THREADS=1 \
        --conf spark.rpc.message.maxSize=1024 \
        --conf spark.driver.maxResultSize=4G \
        --conf spark.locality.wait=0 \
        --conf spark.com.nec.spark.aggregate-on-ve=false \
        --conf spark.com.nec.spark.sort-on-ve=true \
        --conf spark.com.nec.spark.project-on-ve=false \
        --conf spark.com.nec.spark.filter-on-ve=true \
        --conf spark.com.nec.spark.exchange-on-ve=true \
        --conf spark.com.nec.spark.join-on-ve=true \
        --conf spark.com.nec.spark.pass-through-project=false \
        --conf spark.com.nec.spark.fail-fast=false \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.sql.adaptive.coalescePartitions.enabled=true \
        --conf spark.com.nec.spark.amplify-batches=true \
        --conf spark.com.nec.spark.ve.columnBatchSize=512000 \
        --conf spark.com.nec.spark.ve.targetBatchSizeMb=256 \
        --conf spark.sql.inMemoryColumnarStorage.batchSize=512000 
msg = Running Spark Submit using GPU
url = http://localhost:8088/

[VE]
name =  VE
command = $SPARK_HOME/bin/spark-submit \
                --py-files venv_TF_VE.zip \
                --archives venv_TF_VE.zip \
                --master yarn \
                --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH='/usr/local/cuda-11.2/targets/x86_64-linux/lib/:$LD_LIBRARY_PATH' \
                --num-executors=8 --executor-cores=1 --executor-memory=7G --driver-memory=8G \
                --name VE_VE_mode \
                --deploy-mode cluster \
                --jars /opt/cyclone/spark-cyclone-sql-plugin.jar \
                --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON='/usr/local/bin/python3.8' \
                --conf spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON='/usr/local/bin/python3.8' \
                --conf spark.executor.extraClassPath=/opt/cyclone/spark-cyclone-sql-plugin.jar \
                --conf spark.plugins=com.nec.spark.AuroraSqlPlugin \
                --conf spark.com.nec.spark.kernel.directory=/opt/spark/work/cyclone \
                --conf spark.sql.columnVector.offheap.enabled=true \
                --conf spark.executor.resource.ve.amount=1 \
                --conf spark.executor.resource.ve.discoveryScript=/opt/spark/getVEsResources.sh \
                --conf spark.executorEnv.VE_OMP_NUM_THREADS=1 \
                --conf spark.rpc.message.maxSize=1024 \
                --conf spark.driver.maxResultSize=4G \
                --conf spark.locality.wait=0 \
                --conf spark.com.nec.spark.aggregate-on-ve=false \
                --conf spark.com.nec.spark.sort-on-ve=true \
                --conf spark.com.nec.spark.project-on-ve=false \
                --conf spark.com.nec.spark.filter-on-ve=true \
                --conf spark.com.nec.spark.exchange-on-ve=true \
                --conf spark.com.nec.spark.join-on-ve=true \
                --conf spark.com.nec.spark.pass-through-project=false \
                --conf spark.com.nec.spark.fail-fast=false \
                --conf spark.sql.adaptive.enabled=true \
                --conf spark.sql.adaptive.coalescePartitions.enabled=true \
                --conf spark.com.nec.spark.amplify-batches=true \
                --conf spark.com.nec.spark.ve.columnBatchSize=512000 \
                --conf spark.com.nec.spark.ve.targetBatchSizeMb=256 \
                --conf spark.sql.inMemoryColumnarStorage.batchSize=512000 
msg = Running Spark Submit using VE
url = http://localhost:8088/
